{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bin/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'rpn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BrainConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"Brain\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    # For brain dataset, all slices are 240*240\n",
    "    IMAGE_MIN_DIM = 240\n",
    "    IMAGE_MAX_DIM = 240\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_brains(self, data_folder):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # TODO: Update with 10 classes\n",
    "        self.add_class(\"brains\", 0, \"\")\n",
    "        self.add_class(\"brains\", 1, \"\")\n",
    "        self.add_class(\"brains\", 2, \"\")\n",
    "        self.add_class(\"brains\", 3, \"\")\n",
    "        self.add_class(\"brains\", 4, \"\")\n",
    "        self.add_class(\"brains\", 5, \"\")\n",
    "        self.add_class(\"brains\", 6, \"\")\n",
    "        self.add_class(\"brains\", 7, \"\")\n",
    "        self.add_class(\"brains\", 8, \"\")\n",
    "        self.add_class(\"brains\", 9, \"\")\n",
    "        self.add_class(\"brains\", 10, \"\")\n",
    "\n",
    "        # Add images\n",
    "        img_paths = os.listdir(data_folder)\n",
    "        for i in range(len(img_paths)):\n",
    "            self.add_image('brains', image_id = i, path = img_paths[i])\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        path = self.image_info[image_id]['path']\n",
    "        image = np.load(path)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"brains\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGK+zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5808a7f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5805700d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc56eb8b4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc56e922fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /data/user/yuep/segmentation/logs/shapes20181202T1626/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "('In model: ', 'rpn_model')\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/data/bin/anaconda2/lib/python2.7/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 576s 6s/step - loss: 1.9309 - rpn_class_loss: 0.0318 - rpn_bbox_loss: 0.6099 - mrcnn_class_loss: 0.3973 - mrcnn_bbox_loss: 0.4014 - mrcnn_mask_loss: 0.4905 - val_loss: 1.0646 - val_rpn_class_loss: 0.0178 - val_rpn_bbox_loss: 0.5354 - val_mrcnn_class_loss: 0.1445 - val_mrcnn_bbox_loss: 0.1801 - val_mrcnn_mask_loss: 0.1870\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
